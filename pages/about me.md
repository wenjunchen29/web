---
layout              : page-fullwidth
# title               : "About me"
# subheadline         : "ToDo-List &amp; Ideas"
teaser              : 
header:
   image_fullwidth  : "About_me.jpg"
permalink           : "/about me/"
toc: true
#  **<font size="5"><span style="color:blue">Interested in joining the SoBA Lab?</span></font>**
---
<div class="batch">
  <div class="image-container">
    <img class="main-image" src="{{ site.urlimg }}Alex_photo_1.jpg" alt="">
    <div class="overlay">Wenjun Chen (Alex)</div>
  </div>
  <p class="text">I completed my BA in Business English (specialising in linguistics after my sophomore year) at Heilongjiang University. I am doing my 3-year master’s degree in Psycholinguistics at <em>Shanghai International Studies University</em>. My current studies aim to address how people auditorily perceive speech varied from speaker sources (human vs. AI) and prosodies. I used to be a Kungfu player and was an active member of the University Team and had fought in the national championship. I would also spend some of my spare time in the swimming pool. Oh, you can find me tuning in to Live Radios such as <em>Heart London</em> or <em>LBC</em> from time to time, too.  <em>Vikings</em>, <em>Westworld</em>, <em>Le Bureau des Légendes</em>, have made it to my top three favourite TV series so far. I wish to be a globe-trotter and am working on it. <br/> <br/> <a href="https://raw.githubusercontent.com/wenjunchen29/web/main/files/CV_Wenjun_CHEN.pdf" target="_blank" style="text-decoration: underline; text-underline-offset: 3px;">CV</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://scholar.google.co.uk/citations?user=MOPUcx8AAAAJ&hl=zh-TW" target="_blank" style="text-decoration: underline; text-underline-offset: 3px;">Google Scholar</a></p>
</div>

<style>
.batch {
  background-color: lightblue;
  margin: 0px;
  display: flex;
}

.image-container {
  position: relative;
  flex-basis: 40%;
}

.main-image {
  width: 242px; 
  height: 325.5px;
  padding: 10px;
}

.overlay {
  position: absolute;
  bottom: -10px; 
  left: 50px; 
  padding: 5px;
  /* background-color: #113b60; */
  background-color: #5d89ba;
  color: white;
}

.text {
  font-size: 16px;
  margin: 10px;
  flex-basis: 120%;
}
</style>


---

<!-- **Keywords for my academic interest:** AI voice-cloning; Speaker identity; Vocal confidence; AI-generated avatar; Learning; EEG; fMRI -->


[Language is what bonds people in human-human social communication][1]. What happens if it is introduced to human-robot interaction? Smart voice assistant such as Apple’s Siri has been gaining growing popularity among technology users. Additionally, the algorithm is also showcasing its capacity to clone our speaker identity and alter the speech prosody (which is suspected to exert influence over how human-like an AI voice is). As such, I am conducting a series of research to answer some questions through interdisciplinary approaches.

Firstly, I located speaker identity as my research topic and have written a literature review, which is now in press with one of the most influential academic journals in linguistics in China. In this work, I introduced how speaker identity is represented by acoustic cues and influenced by social factors such as communicative intentions. To highlight, I noted how individual speaker identity (who is talking) and speaker group identity (what kind of person is that talker) interplay (Kuhl, 2011; Perrachione et al., 2011). I pointed out future research directions, such as using person perception experimental paradigms to examine how our human brain responds differently to speech produced by human counterparts and AI speakers that share similar individual speaker identities and emotional states. This endeavour provided me with a theoretical framework for my other projects. 
 
Secondly, I am describing human vocal confidence through acoustic features and assessing how AI can mimic human-specific vocal confidence with three research questions: (1) How do acoustic features contribute to depicting human-specific vocal confidence, especially with the vocal-anatomical related cues? (2) Can AI-cloned speakers mimic human beings’ observed vocal confidence encoding mechanism? If yes, then, (3) is predicting confidence levels in human and AI speech across sources viable? This study is under finalising and will be submitted for peer review. 

Thirdly, I am exploring the neuro-correlates of learning and recognising speaker identity in the context of prosodies/sources (group identity being AI or human) variations. I expect this dissertation project to challenge my computational skills, including delivering experiments through PsychoPy and also analyse EEG data with MATLAB and R. ERP analysis, source localisation, time-frequency analysis, and possibly entrainment analysis will be performed. 

Fourth, I am leading a small group of linguistics undergraduates to investigate the social cognition of human and AI speech in two studies. A rating experiment followed by representational similarity matrices (RSM) will be conducted to understand if listeners form different impressions of human and AI speakers. Another speaker identity AX discrimination experiment will also be performed to investigate if the person perception mechanism in human-human interaction also applies to human-robot interaction. 

Fifth, I have some preliminary thoughts about researching AI-generated human avatars, taking factors such as culture, language, emotions, and theory of mind into account. 


**Reference**<br>
Kuhl, P. K. (2011). Who's talking? Science, 333(6042), 529-530.<br> 	
Perrachione, T. K., Del Tufo, S. N., & Gabrieli, J. D. (2011). Human voice recognition depends on language ability. Science, 333(6042), 595-595. 	

  [1]: https://www.youtube.com/watch?v=dctcfxw13AQ






<br>
I’m a second-year postgraduate (2/3 years) student at the Institute of Linguistics, <a href="https://en.wikipedia.org/wiki/Shanghai_International_Studies_University">Shanghai International Studies University</a>. I opted for speech prosody-related psycho- and neuro-linguistics as my research domain for this linguistics degree but didn’t confine my effort only here. For my current Master’s degree, I am conducting a list of research to understand social cognition, particularly when language as a bond in both human-human interaction (HHI) and human-robot interaction (HRI) is involved in the era of algorithms. Among many social cognition domains, I focus on speaker identity, namely, <a href="https://www.science.org/doi/abs/10.1126/science.1210277">‘who is talking’</a>? My postgraduate dissertation is to explore the neuro-correlates of human listeners when they <a href="https://www.jneurosci.org/content/34/33/10821.short"><span style="background-color:yellow">learn and recognise</span></a> the speaker identities they had been trained to be familiar with. In my study, the role the variable speakers’ group identity being human or AI-generated speakers plays will be addressed.
