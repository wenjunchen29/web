---
layout              : page-fullwidth
title               : "About me"
# subheadline         : "ToDo-List &amp; Ideas"
teaser              : 
header:
   image_fullwidth  : "About_me_banner.jpg"
permalink           : "/about me/"
toc: true
#  **<font size="5"><span style="color:blue">Interested in joining the SoBA Lab?</span></font>**
---
<div class="batch">
  <div class="image-container">
    <img class="main-image" src="{{ site.urlimg }}Alex_photo_1.jpg" alt="">
    <div class="overlay">Wenjun Chen (Alex)</div>
  </div>
  <p class="text">I completed my BA in Business English (specialising in linguistics after my sophomore year) at Heilongjiang University. I am doing my 3-year master’s degree in Psycholinguistics at <em>Shanghai International Studies University</em>. My current studies aim to address how people auditorily perceive speech varied from speaker sources (human vs. AI) and prosodies. I used to be a Kungfu player and was an active member of the Sanda (a bit like Muay Thai) Team of Heilongjiang University and had fought in the national championship. I would also spend some of my spare time in the swimming pool. Oh, you can find me tuning in to Live Radios such as <em>Heart London</em> or <em>LBC</em> from time to time, too.  <em>Vikings</em>, <em>Westworld</em>, <em>Le Bureau des Légendes</em>, have made it to my top three favourite TV series so far. I wish to be a globe-trotter and am working on it. <br/> <br/> <a href="https://raw.githubusercontent.com/wenjunchen29/web/main/files/CV_Wenjun_CHEN.pdf" target="_blank" style="text-decoration: underline; text-underline-offset: 3px;">CV</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://scholar.google.co.uk/citations?user=MOPUcx8AAAAJ&hl=zh-TW" target="_blank" style="text-decoration: underline; text-underline-offset: 3px;">Google Scholar</a></p>
</div>

<style>
.batch {
  background-color: lightblue;
  margin: 0px;
  display: flex;
}

.image-container {
  position: relative;
  flex-basis: 40%;
}

.main-image {
  width: 242px; 
  height: 325.5px;
  padding: 10px;
}

.overlay {
  position: absolute;
  bottom: -10px; 
  left: 50px; 
  padding: 5px;
  /* background-color: #113b60; */
  background-color: #5d89ba;
  color: white;
}

.text {
  font-size: 16px;
  margin: 10px;
  flex-basis: 120%;
}
</style>


---

**Keywords for my academic interest:** AI voice-cloning; Speaker identity; Vocal confidence; AI-generated avatar; Learning; EEG; fMRI

I’m a second-year postgraduate (2/3 years) student at the Institute of Linguistics, <a href="https://en.wikipedia.org/wiki/Shanghai_International_Studies_University">Shanghai International Studies University</a>. I opted for speech prosody-related psycho- and neuro-linguistics as my research domain for this linguistics degree but didn’t confine my effort only here. For my current Master’s degree, I am conducting a list of research to understand social cognition, particularly when language as a bond in both human-human interaction (HHI) and human-robot interaction (HRI) is involved in the era of algorithms. Among many social cognition domains, I focus on speaker identity, namely, <a href="https://www.science.org/doi/abs/10.1126/science.1210277">‘who is talking’</a>? My postgraduate dissertation is to explore the neuro-correlates of human listeners when they <a href="https://www.jneurosci.org/content/34/33/10821.short"><span style="background-color:yellow">learn and recognise</span></a> the speaker identities they had been trained to be familiar with. In my study, the role the variable speakers’ group identity being human or AI-generated speakers plays will be addressed.

As a foundation, I have prepared audio clips by both humans and the corresponding AI-cloned speakers; altogether, 24 speakers * 2 sources being human or AI * 219 sentences * 3 confidence levels being confident, doubtful or neutral = 31,356 sentences in wav form. First thing first, I need to ensure the comparability of speech by human and AI speakers in terms of speaker identity and vocal confidence. I am validating the success of the AI algorithm’s speaker identity through a technique - speaker embedding that extracts a vector representation from a speech signal that captures the speaker identity of each speaker and presents them in a 2-D plotting. I am confirming AI’s capacity to clone not only speaker identity but also human-specific vocal confidence that carries pragmatic intention through a combination of studies: acoustic analysis and machine learning classification; for this, I have incorporated the findings into a prepared manuscript and will post it for peer-review soon. 

With the validated human and AI voices, I move to social cognition by naive human listeners, which is to be performed from May 2023 to May 2024. Firstly, how do speech prosodies mediate people’s perception of AI and human voices? This is to be reflected in the behavioural responses on the 7-Likert scale. I will be circling around ‘speaker identity’ and performing two studies. So, secondly, do people process human and AI identities in a similar manner? This is to be answered with the classic AxB paradigm while also taking prosodies into account. Then it comes to my dissertation, in which I will employ the training-testing paradigm, where listeners would learn the identity of a specific speaker and be tested on their learning outcomes. Here, I will use data from the electroencephalogram (EEG) to perform ERP analysis, source localisation, time-frequency analysis, and possibly entrainment analysis. 

Since I’m currently working on only the modality of auditory, I do value the possibility of moving to multimodality. And this leads me to my initial interest in AI-generated human avatars, which has been seen and expected to see, thanks to the prevalence of the game changer - ChatGPT, increasing applications in our human society. But less is known about what challenges it might bring to human beings’ social cognition patterns in social interaction. Also, I suppose exposures and interactions, as two learning engagements, might play a part that shapes’ a human being’s specific social cognition of these avatars. Yet, these are merely preliminary thoughts subject to changes and await experimental evidence and theoretical support. 

Click for my CV, biography, and more details about my current lab affiliation and research interests publications.


---
