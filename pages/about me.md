---
layout              : page-fullwidth
# title               : "About me"
# subheadline         : "ToDo-List &amp; Ideas"
teaser              : 
header:
   image_fullwidth  : "About_me.jpg"
# permalink           : "/about me/"
# permalink           : "/index/"
permalink: /index.html


toc: true
#  **<font size="5"><span style="color:blue">Interested in joining the SoBA Lab?</span></font>**
---
<div class="batch">
  <div class="image-container">
    <img class="main-image" src="{{ site.urlimg }}Alex_photo_1.jpg" alt="">
    <div class="overlay">Wenjun Chen (Alex)</div>
  </div>
  <p class="text">I completed my BA in Business English (specialising in linguistics after my sophomore year) at Heilongjiang University. I am doing my 3-year master’s degree in Psycholinguistics at <em>Shanghai International Studies University</em>. My current studies aim to address how people auditorily perceive speech varied from speaker sources (human vs. AI) and prosodies. I used to be a Kungfu player and was an active member of the University Team and had fought in the national championship. I would also spend some of my spare time in the swimming pool. Oh, you can find me tuning in to Live Radios such as <em>Heart London</em> or <em>LBC</em> from time to time, too.  <em>Vikings</em>, <em>Westworld</em>, <em>Le Bureau des Légendes</em>, have made it to my top three favourite TV series so far. I wish to be a globe-trotter and am working on it. <br/> <br/> <a href="https://raw.githubusercontent.com/wenjunchen29/web/main/files/CV_Wenjun_Chen.pdf" target="_blank" style="text-decoration: underline; text-underline-offset: 3px;">CV</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://scholar.google.co.uk/citations?user=MOPUcx8AAAAJ&hl=zh-TW" target="_blank" style="text-decoration: underline; text-underline-offset: 3px;">Google Scholar</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://twitter.com/wenjunchen_alex" target="_blank" style="text-decoration: underline; text-underline-offset: 3px;">Twitter</a></p>
  
</div>

<style>
.batch {
  background-color: lightblue;
  margin: 0px;
  display: flex;
}

.image-container {
  position: relative;
  flex-basis: 40%;
}

.main-image {
  width: 242px; 
  height: 325.5px;
  padding: 10px;
}

.overlay {
  position: absolute;
  bottom: -10px; 
  left: 50px; 
  padding: 5px;
  /* background-color: #113b60; */
  background-color: #5d89ba;
  color: white;
}

.text {
  font-size: 16px;
  margin: 10px;
  flex-basis: 120%;
}
</style>


---

<!-- **Keywords for my academic interest:** AI voice-cloning; Speaker identity; Vocal confidence; AI-generated avatar; Learning; EEG; fMRI -->

<div style="width: 70%; margin: auto;">

Language is what bonds people in human-human social communication. <a href="https://www.youtube.com/watch?v=dctcfxw13AQ" target="_blank"><span style="color:blue">But what happens if it is introduced to human-robot interaction?</span></a> Smart voice assistant such as Apple’s Siri has been gaining growing popularity among technology users. Additionally, the algorithm is also showcasing its capacity to clone our speaker identity and alter the speech prosody (which is suspected to exert influence over how human-like an AI voice is). As such, I am conducting a series of research to answer some questions through interdisciplinary approaches.
<br><br>
Firstly, I located <a href="https://www.science.org/doi/10.1126/science.1210277" target="_blank"><span style="color:blue">speaker identity</span></a> as my research topic and have written a literature review, which is now in press with one of the most influential academic journals in linguistics in China. In this work, I introduced how speaker identity is represented by acoustic cues and influenced by social factors such as communicative intentions. To highlight, I noted how individual speaker identity (who is talking) and speaker group identity (what kind of person is that talker) interplay. I pointed out future research directions, such as using person perception experimental paradigms to examine how our human brain responds differently to speech produced by human counterparts and AI speakers that share similar individual speaker identities and emotional states. This endeavour provided me with a theoretical framework for my other projects. 
 <br><br>
Secondly, I am describing human vocal confidence through acoustic features and assessing how AI can mimic <a href="https://psycnet.apa.org/doi/10.1016/j.specom.2017.01.011" target="_blank"><span style="color:blue">human-specific vocal confidence</span></a> with three research questions: (1) How do acoustic features contribute to depicting human-specific vocal confidence, especially with the vocal-anatomical related cues? (2) Can AI-cloned speakers mimic human beings’ observed vocal confidence encoding mechanism? If yes, then, (3) is predicting confidence levels in human and AI speech across sources viable? This study is under finalising and will be submitted for peer review. 
<br><br>
Thirdly, I am exploring the neuro-correlates of <a href="https://www.jneurosci.org/content/34/33/10821" target="_blank"><span style="color:blue">learning and recognising</span></a> speaker identity in the context of prosodies/sources (group identity being AI or human) variations. I expect this dissertation project to challenge my computational skills, including delivering experiments through PsychoPy and also analyse EEG data with MATLAB and R. ERP analysis, source localisation, time-frequency analysis, and possibly entrainment analysis will be performed. 
<br><br>
Fourth, I am leading a small group of linguistics undergraduates to investigate the social cognition of human and AI speech in two studies. A rating experiment followed by representational similarity matrices (RSM) will be conducted to understand if listeners form different <a href="https://www.sciencedirect.com/science/article/abs/pii/S074756320200081X" target="_blank"><span style="color:blue">impressions of human and AI speakers</span></a>. Another speaker identity <a href="https://www.pnas.org/doi/full/10.1073/pnas.1401383111" target="_blank"><span style="color:blue">AX discrimination</span></a> experiment will also be performed to investigate if the person perception mechanism in human-human interaction also applies to human-robot interaction.


<br><br>
Fifth, I have some preliminary thoughts about researching <a href="https://www.youtube.com/watch?v=U0HNdsxC8YU&t=248s" target="_blank"><span style="color:blue">AI-generated human avatars</span></a> powered by ChatGPT, taking factors such as culture, language, emotions, and theory of mind into account. EEG, Eye-tracking, and fMRI can be possible research tools. <br>

<a href="/web/AI_avatar_and_learning_sciences/">Possible questions about learning sciences can be</a>, for example, how does AI avatar in video affect learners’ (1) emotional regulation and resilience in stressful learning situations, (2) <a href="https://journals.sagepub.com/doi/full/10.1177/00336882231162868" target="_blank"><span style="color:blue">language acquisition</span></a> and communication skills in multilingual learning environments, (3) cultural identity and sense of belonging in diverse learning communities, (4) perspective-taking and empathy in collaborative learning tasks, and (5) creativity and self-expression in artistic learning projects?
</div>





----
<span style="color:green">Note. This website is not necessarily perfect and is under developing. Viewing it with browser on PC is recommended.</span>
