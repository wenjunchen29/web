---
layout              : page-fullwidth
# title               : "About me"
# subheadline         : "ToDo-List &amp; Ideas"
teaser              : 
header:
   image_fullwidth  : "About_me.jpg"
permalink           : "/about me/"
toc: true
#  **<font size="5"><span style="color:blue">Interested in joining the SoBA Lab?</span></font>**
---
<div class="batch">
  <div class="image-container">
    <img class="main-image" src="{{ site.urlimg }}Alex_photo_1.jpg" alt="">
    <div class="overlay">Wenjun Chen (Alex)</div>
  </div>
  <p class="text">I completed my BA in Business English (specialising in linguistics after my sophomore year) at Heilongjiang University. I am doing my 3-year master’s degree in Psycholinguistics at <em>Shanghai International Studies University</em>. My current studies aim to address how people auditorily perceive speech varied from speaker sources (human vs. AI) and prosodies. I used to be a Kungfu player and was an active member of the University Team and had fought in the national championship. I would also spend some of my spare time in the swimming pool. Oh, you can find me tuning in to Live Radios such as <em>Heart London</em> or <em>LBC</em> from time to time, too.  <em>Vikings</em>, <em>Westworld</em>, <em>Le Bureau des Légendes</em>, have made it to my top three favourite TV series so far. I wish to be a globe-trotter and am working on it. <br/> <br/> <a href="https://raw.githubusercontent.com/wenjunchen29/web/main/files/CV_Wenjun_CHEN.pdf" target="_blank" style="text-decoration: underline; text-underline-offset: 3px;">CV</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://scholar.google.co.uk/citations?user=MOPUcx8AAAAJ&hl=zh-TW" target="_blank" style="text-decoration: underline; text-underline-offset: 3px;">Google Scholar</a></p>
</div>

<style>
.batch {
  background-color: lightblue;
  margin: 0px;
  display: flex;
}

.image-container {
  position: relative;
  flex-basis: 40%;
}

.main-image {
  width: 242px; 
  height: 325.5px;
  padding: 10px;
}

.overlay {
  position: absolute;
  bottom: -10px; 
  left: 50px; 
  padding: 5px;
  /* background-color: #113b60; */
  background-color: #5d89ba;
  color: white;
}

.text {
  font-size: 16px;
  margin: 10px;
  flex-basis: 120%;
}
</style>


---

**Keywords for my academic interest:** AI voice-cloning; Speaker identity; Vocal confidence; AI-generated avatar; Learning; EEG; fMRI

Language is what bonds people in human-human social communication. What happens if it was introduced to human-robot interaction? Smart voice assistant such as Apple’s Siri has been gaining growing popularity among technology users. Additionally, the algorithm is also showcasing its capacity to clone our speaker identity and alter the speech prosody (which is suspected to exert influence over how human-like an AI voice is). As such, I am conducting a series of research to answer some questions through interdisciplinary approaches. 

Firstly, I located speaker identity as my research topic and have written a literature review, which is now in press with one of the most influential academic journals in linguistics in China. In this work, I introduced how speaker identity is represented by acoustic cues and influenced by social factors such as communicative intentions. To highlight, I noted how individual speaker identity (who is talking) and speaker group identity (what kind of person is that talker) interplay. I pointed out future research directions, such as using person perception experimental paradigms to examine how our human brain responds differently to speech produced by human counterparts and AI speakers that share similar individual speaker identities and emotional states. This endeavour provided me with a theoretical framework for my other projects. 
 
Secondly, I am describing human vocal confidence through acoustic features and assessing how AI can mimic human-specific vocal confidence with three research questions: (1) How do acoustic features contribute to depicting human-specific vocal confidence, especially with the vocal-anatomical related cues? (2) Can AI-cloned speakers mimic human beings’ observed vocal confidence encoding mechanism? If yes, then, (3) is predicting confidence levels in human and AI speech across sources viable? This study is finalising and will be submitted for peer review. 

Thirdly, I am exploring the learning and recognising neuro-correlates of speaker identity in the context of prosody/speaker group variation. I expect this dissertation project to systematically challenge my electroencephalogram (EEG) data analysis skills, including ERP analysis, source localisation, time-frequency analysis, and possibly entrainment analysis.

<br>
I’m a second-year postgraduate (2/3 years) student at the Institute of Linguistics, <a href="https://en.wikipedia.org/wiki/Shanghai_International_Studies_University">Shanghai International Studies University</a>. I opted for speech prosody-related psycho- and neuro-linguistics as my research domain for this linguistics degree but didn’t confine my effort only here. For my current Master’s degree, I am conducting a list of research to understand social cognition, particularly when language as a bond in both human-human interaction (HHI) and human-robot interaction (HRI) is involved in the era of algorithms. Among many social cognition domains, I focus on speaker identity, namely, <a href="https://www.science.org/doi/abs/10.1126/science.1210277">‘who is talking’</a>? My postgraduate dissertation is to explore the neuro-correlates of human listeners when they <a href="https://www.jneurosci.org/content/34/33/10821.short"><span style="background-color:yellow">learn and recognise</span></a> the speaker identities they had been trained to be familiar with. In my study, the role the variable speakers’ group identity being human or AI-generated speakers plays will be addressed.

As a foundation, I have prepared audio clips by both humans and the corresponding AI-cloned speakers; altogether, 24 speakers * 2 sources being human or AI * 219 sentences * 3 confidence levels being confident, doubtful or neutral = 31,356 sentences in wav form. First thing first, I need to ensure the comparability of speech by human and AI speakers in terms of speaker identity and vocal confidence. I am validating the success of the AI algorithm’s speaker identity through a technique - speaker embedding that extracts a vector representation from a speech signal that captures the speaker identity of each speaker and presents them in a 2-D plotting. I am confirming AI’s capacity to clone not only speaker identity but also human-specific vocal confidence that carries pragmatic intention through a combination of studies: acoustic analysis and machine learning classification; for this, I have incorporated the findings into a prepared manuscript and will post it for peer-review soon. 

With the validated human and AI voices, I move to social cognition by naive human listeners, which is to be performed from May 2023 to May 2024. Firstly, how do speech prosodies mediate people’s perception of AI and human voices? This is to be reflected in the behavioural responses on the 7-Likert scale. I will be circling around ‘speaker identity’ and performing two studies. So, secondly, do people process human and AI identities in a similar manner? This is to be answered with the classic AxB paradigm while also taking prosodies into account. Then it comes to my dissertation, in which I will employ the training-testing paradigm, where listeners would learn the identity of a specific speaker and be tested on their learning outcomes. Here, I will use data from the electroencephalogram (EEG) to perform ERP analysis, source localisation, time-frequency analysis, and possibly entrainment analysis. 

Since I’m currently working on only the modality of auditory, I do value the possibility of moving to multimodality. And this leads me to my initial interest in AI-generated human avatars, which has been seen and expected to see, thanks to the prevalence of the game changer - ChatGPT, increasing applications in our human society. But less is known about what challenges it might bring to human beings’ social cognition patterns in social interaction. Also, I suppose exposures and interactions, as two learning engagements, might play a part that shapes’ a human being’s specific social cognition of these avatars. Yet, these are merely preliminary thoughts subject to changes and await experimental evidence and theoretical support. 

---
