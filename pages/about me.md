---
layout              : page-fullwidth
# title               : "About me | Wenjun Chen"
# subheadline         : "ToDo-List &amp; Ideas"
teaser              : 
header:
   image_fullwidth: "About_me.jpg"
# permalink           : "/about me/"
# permalink           : "/index/"
permalink: /index.html


toc: true
#  **<font size="5"><span style="color:blue">Interested in joining the SoBA Lab?</span></font>**
---
<div class="batch">
  <div class="image-container">
    <img class="main-image" src="{{ site.url }}{{ site.baseurl }}/images/Alex_photo_1.jpg" alt="">
    <div class="overlay">Wenjun Chen (Alex)</div>
  </div>
  <p class="text">I completed my BA in Business English (specialising in Linguistics) at Heilongjiang University. I am doing my 3-year master’s degree in Master of Foreign Linguistics and Applied Linguistics (Psycholinguistics & Neurolinguistics) at <a href="https://xiaomingjiang.wordpress.com/about/" target="_blank">the Psychology and Neurobiology of Language Group</a> based at Shanghai International Studies University. My current studies aim to address how people auditorily process speech, which varies from speaker sources (human vs. AI) and prosodies (confident/doubtful/neutral-intending) and the according neuro correlates. <br/> <br/> <a href="{{ site.url }}{{ site.baseurl }}/files/CV_Wenjun_CHEN.pdf" target="_blank"><span style="text-decoration: underline; text-underline-offset: 3px;">CV</span></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://scholar.google.co.uk/citations?user=MOPUcx8AAAAJ&hl=zh-TW" target="_blank" style="text-decoration: underline; text-underline-offset: 3px;">Google Scholar</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://twitter.com/wenjunchen_alex" target="_blank" style="text-decoration: underline; text-underline-offset: 3px;">Twitter</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a style="text-decoration: underline; text-underline-offset: 3px;">Email: </a>chenwenjunedu@outlook.com</p> 

  

</div>

<style>
.batch {
  background-color: lightblue;
  margin: 0px;
  display: flex;
}

.image-container {
  position: relative;
  flex-basis: 40%;
}

.main-image {
  width: 242px; 
  height: 325.5px;
  padding: 10px;
}

.overlay {
  position: absolute;
  bottom: -10px; 
  left: 50px; 
  padding: 5px;
  /* background-color: #113b60; */
  background-color: #5d89ba;
  color: white;
}

.text {

  font-size: 16px;
  margin: 10px;
  flex-basis: 120%;
}
</style>


---

<!-- **Keywords for my academic interest:** AI voice-cloning; Speaker identity; Vocal confidence; AI-generated avatar; Learning; EEG; fMRI -->

<div style="width: 70%; margin: auto;">

Below introduces my endeavours to earn my Master's degree in Foreign Linguistics and Applied Linguistics. Language is what bonds people in human-human social communication. <a href="https://www.youtube.com/watch?v=dctcfxw13AQ" target="_blank"><span style="color:blue">But what happens if it is introduced to human-robot interaction?</span></a> Smart voice assistant such as Apple’s Siri has been gaining growing popularity among technology users. Additionally, the algorithm is also showcasing its capacity to clone our speaker identity and alter the speech prosody (which is suspected to exert influence over how human-like an AI voice is). As such, I am conducting a series of research to answer some questions through interdisciplinary approaches.
<br><br>
Firstly, I located <a href="https://www.science.org/doi/10.1126/science.1210277" target="_blank"><span style="color:blue">speaker identity</span></a> as my research topic for my postgraduate degree and have written a literature review, which is now in press with one of the most influential academic journals in linguistics in China. In this work, I introduced how speaker identity is represented by acoustic cues and influenced by social factors such as communicative intentions. To highlight, I noted how individual speaker identity (who is talking) and speaker group identity (what kind of person is that talker) interplay. I pointed out future research directions, such as using person perception experimental paradigms to examine how our human brain responds differently to speech produced by human counterparts and AI speakers that share similar individual speaker identities and emotional states. This endeavour provided me with a theoretical framework for my other projects. 
 <br><br>
Secondly, I am describing human vocal confidence through acoustic features and assessing how AI can mimic <a href="https://psycnet.apa.org/doi/10.1016/j.specom.2017.01.011" target="_blank"><span style="color:blue">human-specific vocal confidence</span></a> with three research questions: (1) How do acoustic features contribute to depicting human-specific vocal confidence, especially with the vocal-anatomical related cues? (2) Can AI-cloned speakers mimic human beings’ observed vocal confidence encoding mechanism? If yes, then, (3) is predicting confidence levels in human and AI speech across sources viable (with machine learning)? This study has been submitted for peer review. 
<br><br>
Thirdly, I am exploring the neuro-correlates of <a href="https://www.jneurosci.org/content/34/33/10821" target="_blank"><span style="color:blue">learning and recognising</span></a> speaker identity in the context of prosodies/sources (group identity being AI or human) variations. I expect this dissertation project to challenge my computational skills, including delivering experiments through PsychoPy and also analyse EEG data with MATLAB and R. ERP analysis, source localisation, time-frequency analysis, and possibly entrainment analysis will be performed. 
<br><br>
Fourth, I am leading a small group of linguistics undergraduates to investigate the social cognition of human and AI speech in two studies. A rating experiment followed by analysis including representational similarity matrices (RSM), principal component analysis (PCA), and machine learning will be conducted to understand if listeners form different <a href="https://www.sciencedirect.com/science/article/abs/pii/S074756320200081X" target="_blank"><span style="color:blue">impressions of human and AI speakers</span></a>. 
 <br><br>

Fifth, another speaker identity <a href="https://www.pnas.org/doi/full/10.1073/pnas.1401383111" target="_blank"><span style="color:blue">AX discrimination</span></a> experiment will also be performed to investigate if the person perception mechanism in human-human interaction also applies to human-robot interaction.


</div>





----
<span style="color:green">Note. This website is under developing. Viewing it with browser on PC is recommended. And, thank you for checking my website! Have a lovely one!</span>
